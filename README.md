# AI Log PoC - Steps Done

Overview

This project is a **Proof of Concept (PoC)** to analyze system logs and detect anomalies. 
The goal is to explore whether AI can help troubleshoot issues faster and identify potential root causes in complex log files, such as those generated by Genesys applications.

We used sample Hadoop logs to simulate the workflow before considering specific application logs in a test environment.

Phase 1: Setup and GitHub

## Log Data Used
Hadoop logs from LogHub were used as a substitute for the target application logs to safely test the AI log analysis workflow. They have a similar structure with timestamps, log levels, components, and messages with the target application that needs proof of Concept validated. This allows us to practice parsing, anomaly detection, and visualization before working with real application logs in a test environement.

## What I did so far
1. **Created project folder** on my Mac (`~/Desktop/AI_Log_PoC`).  
2. **Installed Python 3.12** and Jupyter Notebook to run code.  
3. **Collected logs**:  
   - Copied Mac system logs for practice.  
   - Downloaded a public, anonymized Hadoop log dataset from LogHub (GitHub).  
4. **Loaded logs into a notebook** and parsed important fields:  
   - Timestamp  
   - Log level (INFO, WARN, ERROR)  
   - Thread  
   - Component  
   - Message  
5. **Ran basic anomaly detection** using Python (Isolation Forest) to find unusual log entries.  
6. **Visualized results** with simple tables and charts.  
7. **Initialized Git repository** locally and pushed all files to GitHub.  
8. **Created a README** to document steps and next actions.  

## Next Steps
- Improve anomaly detection with **template rarity** (rare log patterns).  
- Add more **features** for machine learning (capital words, message length, special characters).  
- Generate **better visualizations** of anomalies.  
- Use this project as example and foundational work for replicating the process in a possible test environement to handle **specific application logs** when they become available.


Phase 2: Anomaly Detection

In this phase, the PoC applies AI-based anomaly detection on structured Hadoop sample logs. Using a machine learning model (Isolation Forest), the notebook identifies unusual log events, including errors and unexpected behaviors. The model successfully flagged the top anomalies and visualized them over time, demonstrating that AI can help highlight potential root causes in log data. This confirms the feasibility of using AI to assist in log analysis and troubleshooting, forming a solid proof of concept for future work with live application logs.

**Results / Proof of Concept**

Top anomalies detected: Errors from RMCommunicator Allocator in Hadoop logs.

Visualization: Shows unusual log events over time.

Conclusion: AI successfully identifies anomalous log entries, confirming the feasibility of using AI to assist in log analysis and troubleshooting.

## Safety Notes

All commands and code used in this PoC are purely analytical and do not execute any log data as code or connect to external endpoints (except GitHub for repository storage). 

üîπ Git / GitHub Commands

These commands were used to set up version control and upload the project to GitHub.
They only affect the project folder, nothing else.

git init ‚Üí starts Git tracking in the folder

git add . ‚Üí marks files to be included in the next commit

git commit -m "message" ‚Üí saves a snapshot of changes locally

git branch -M main ‚Üí renames the branch to main

git remote add origin <URL> ‚Üí connects the project to GitHub

git pull origin main --rebase ‚Üí Updates your local branch with remote changes

git push -u origin main ‚Üí uploads files to GitHub

üîπ Jupyter / Python Commands

These commands were used to read and test the Hadoop sample logs.
They only worked on the local file and did not change the logs themselves.

Importing libraries

import pandas as pd ‚Üí loads the Pandas library for data handling

from sklearn.ensemble import IsolationForest ‚Üí loads the anomaly detection model

Reading the log file

pd.read_csv("Hadoop_2k.log_structured.csv") ‚Üí opens the log file

logs.head() ‚Üí shows the first rows of data

Formatting and exploring

pd.to_datetime(...) ‚Üí converts timestamps into date format

logs.describe() ‚Üí quick summary (counts, averages, etc.)

Running anomaly detection

IsolationForest(...) ‚Üí sets up the anomaly detection model

model.fit(...) ‚Üí trains the model on selected columns

model.predict(...) ‚Üí adds a column marking ‚Äúnormal‚Äù vs ‚Äúanomaly‚Äù

logs[logs['anomaly'] == -1] ‚Üí shows only anomalies

import pandas as pd
Loads the pandas library for data manipulation. 

import numpy as np
Loads numpy for numeric computations. 

from sklearn.ensemble import IsolationForest
Imports the machine learning model used for anomaly detection. Safe.

import matplotlib.pyplot as plt
Loads matplotlib for visualizing anomalies. 

logs = pd.read_csv("loghub/Hadoop/Hadoop_2k.log_structured.csv")
Reads structured log file into memory. Read-only. 

logs['timestamp'] = pd.to_datetime(logs['Date'] + ' ' + logs['Time'])
Combines Date + Time columns into a datetime object.

logs['timestamp_num'] = logs['timestamp'].astype(int) / 10**9
Converts datetime to seconds since epoch. 

level_map = {'INFO': 0, 'WARN': 1, 'ERROR': 2}
logs['level_num'] = logs['Level'].map(level_map)
Converts log levels to numeric for ML analysis. 

model = IsolationForest(contamination=0.05, random_state=42)
Initializes the anomaly detection model.

logs['anomaly'] = model.fit_predict(logs[['timestamp_num', 'level_num']])
Runs anomaly detection. Adds a new column with predictions. Safe; non-destructive.

logs[logs['anomaly'] == -1].head(10)
Displays the top 10 anomalous log entries; read-only.

plt.scatter(...)
Generates visualizations for anomalies; only plots data.

üëâ Note:

All Git commands just handle saving and uploading the project.

All Python commands only read and analyze the local CSV log file.

No external systems, no sensitive data touched.

x
